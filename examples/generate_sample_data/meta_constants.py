# OPERATORS_LIST = {
#     'Custom': {
#         'description': "A single agent with a customizable prompt, which is used in general objective.",
#         'input': {
#             'input': {
#                 'type': 'string',
#                 'desc': 'The string describing the problem that needs to be solved.',
#             },
#             'instruction': {
#                 'type': 'string',
#                 'desc': 'The instruction of addressing this problem.'
#             }
#         },
#         'interface': 'self.custom(input="", instruction="")',
#         'output': {
#             'response': {
#                 'type': 'string',
#                 'desc': 'The response generated by the agent.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.custom = operator.Custom(self.llm)

#     async def __call__(self, problem: str):
#         # Step x: .....
#         solution = await self.custom(input=problem, instruction="")
        
#         step_x_response = solution['response']
#         """
#     },
#     'AnswerGenerate': {
#         'description': "Generate a direct answer based on the given question or input text using the predefined answer generation prompt.",
#         'input': {
#             'input': {
#                 'type': 'string',
#                 'desc': 'The question or problem statement to be answered.'
#             }
#         },
#         'interface': 'self.answer_generate(input="")',
#         'output': {
#             'thought': {
#                 'type': 'string',
#                 'desc': 'The thought process or reasoning behind the generated answer.'
#             },
#             'answer': {
#                 'type': 'string',
#                 'desc': 'The generated direct answer to the question or input text.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.answer_generate = operator.AnswerGenerate(self.llm)

#     async def __call__(self, problem: str):
#         # Step x: .....
#         solution = await self.answer_generate(input=problem)
        
#         step_x_thought = solution['thought']
#         step_x_answer = solution['answer']
#         """
#     },
#     'CustomCodeGenerate': {
#         'description': "Generate executable source code from a problem description and an entry point specification, following custom instructions.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The problem description or context for the code generation task.'
#             },
#             'entry_point': {
#                 'type': 'string',
#                 'desc': 'The name of the entry point function to be implemented in the generated code.'
#             },
#             'instruction': {
#                 'type': 'string',
#                 'desc': 'Instructions or prompts for the code generation task.'
#             }
#         },
#         'interface': 'self.custom_code_generate(problem="", entry_point="", instruction="")',
#         'output': {
#             'code': {
#                 'type': "string",
#                 'desc': "The generated Python executable source code."
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.custom_code_generate = operator.CustomCodeGenerate(self.llm)

#     async def __call__(self, problem: str):
#         # Step x: .....
#         solution = await self.custom_code_generate(problem=problem, entry_point="solve", instruction="")
        
#         step_x_code = solution['code']
#         """
#     },
#     'ScEnsemble': {
#         'description': "Implements Self-Consistency (SC) Ensemble reasoning â€” aggregates multiple reasoning paths and selects the most consistent or common solution.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The problem statement or question for which SC ensemble reasoning is applied.'
#             },
#             'solutions': {
#                 'type': 'list[str]',
#                 'desc': 'A list of candidate solutions or reasoning paths for the problem.'
#             }
#         },
#         'interface': 'self.sc_ensemble(solutions=[], problem="")',
#         'output': {
#             'thought': {
#                 'type': 'string',
#                 'desc': 'The thought process or reasoning behind the selected solution.'
#             },
#             'sc_solution': {
#                 'type': 'string',
#                 'desc': 'The selected most consistent or common solution.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.sc_ensemble = operator.ScEnsemble(self.llm)
#         self.answer_generate = operator.AnswerGenerate(self.llm)

#     async def __call__(self, problem: str):
    
#         solutions = []
        
#         for i in range(3):
#             # Step x - 1: .....
#             solution = await self.answer_generate(input=problem)
#             step_x-1_thought = solution['thought']
#             step_x-1_answer = solution['answer']
            
#             solutions = solutions + [step_x-1_answer]
    
#         # Step x: .....
#         solution = await self.sc_ensemble(problem=problem, solutions=solutions)
        
#         step_x_thought = solution['thought']
#         step_x_sc_solution = solution['sc_solution']
#         """
#     },
#     'Programmer': {
#         'description': "A reasoning agent that generates, executes, and iteratively improves Python code solutions through feedback loops and retry mechanisms.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The problem or coding task to be solved.'
#             },
#             'analysis': {
#                 'type': 'string',
#                 'desc': 'Optional contextual analysis or explanation of the problem to guide generation.'
#             }
#         },
#         'interface': 'self.programmer(problem="", analysis="None")',
#         'output': {
#             'code': {
#                 'type': 'string',
#                 'desc': 'The generated Python code solution.'
#             },
#             'output': {
#                 'type': 'string',
#                 'desc': 'The output or result of executing the generated code.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.programmer = operator.Programmer(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         solution = await self.programmer(problem=problem, analysis="")
        
#         step_x_code = solution['code']
#         step_x_output = solution['output']
#         """
#     },
#     'Test': {
#         'description': "Executes public test cases for a given solution. If tests fail, the model reflects on the errors and revises the code until it passes or the retry limit is reached.",
#         'input': {
#             'problem': 'The original problem or coding task.',
#             'solution': 'The candidate Python code solution to be tested.',
#             'entry_point': 'The name of the function to be executed in the test environment.'
#         },
#         'interface': 'self.test(problem="", solution="", entry_point="")',
#         'output': "An object containing the test 'result' (True/False) and possibly a revised 'solution'."
#     },
#     'Format': {
#         'description': "Formats a generated solution into a clean and well-presented format (e.g., readable structure, docstring, or comment formatting).",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The original problem or coding task.'
#             },
#             'solution': {
#                 'type': 'string',
#                 'desc': 'The unformatted or raw generated solution.'
#             }
#         },
#         'interface': 'self.format(problem="", solution="")',
#         'output': {
#             'solution': {
#                 'type': 'string',
#                 'desc': 'The formatted and polished solution text.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.format = operator.Format(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         solution = await self.format(problem=problem, solution=".....")
        
#         step_x_solution = solution['solution']
#         """
#     },
#     'Review': {
#         'description': "Performs critical review on a given solution, evaluating its correctness, completeness, and efficiency, and producing structured feedback.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The original problem statement.'
#             },
#             'solution': {
#                 'type': 'string',
#                 'desc': 'The proposed solution or reasoning to review.'  
#             }
#         },
#         'interface': 'self.review(problem="", solution="")',
#         'output': {
#             'review_result': {
#                 'type': 'boolean',
#                 'desc': 'The Review Result (Bool). If you think this solution looks good for you, return "true"; If not, return "false".'
#             },
#             'feedback': {
#                 'type': 'string',
#                 'desc': 'Your FeedBack for this problem based on the criteria. If the review result is true, you can put it "nothing here".'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.review = operator.Review(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         solution = await self.review(problem=problem, solution=".....")
        
#         step_x_review_result = solution['review_result']
#         step_x_feedback = solution['feedback']
#         """
#     },
#     'Revise': {
#         'description': "Refines a solution based on feedback from the review process to produce an improved version addressing identified issues.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The original problem statement.'  
#             },
#             'solution': {
#                 'type': 'string',
#                 'desc': 'The proposed solution or reasoning to refine.'
#             },
#             'feedback':{
#                 'type': 'string',
#                 'desc': 'Reviewer or critic feedback indicating issues or improvements.'
#             }
#         },
#         'interface': 'self.revise(problem="", solution="", feedback="")',
#         'output': {
#             'solution': {
#                 'type': 'string',
#                 'desc': 'Based on the feedback, revised solution for this problem'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.revise = operator.Revise(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         solution = await self.revise(problem=problem, solution=".....", feedback="....")
        
#         step_x_solution = solution['solution']
#         """
#     },
#     'MdEnsemble': {
#         'description': "Implements majority voting (multi-decision ensemble) reasoning by sampling multiple model outputs and selecting the most frequently chosen answer.",
#         'input': {
#             'solutions': 'A list of candidate answers or solutions generated by different agents or runs.',
#             'problem': 'The problem statement or question for which ensemble voting is applied.',
#             'mode': 'Optional XML/text output mode.'
#         },
#         'interface': 'self.md_ensemble(solutions=[], problem="")',
#         'output': 'The final selected solution after majority voting aggregation.'
#     },
#     'Debater': {
#         'description': "An agent who argues for and against proposed solutions from other debaters to refine reasoning and reveal the most robust argument.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The topic or problem under debate.'
#             },
#             'proposed_solutions': {
#                 'type': 'list[str]',
#                 'desc': 'The proposed solutions or arguments from other debaters.'
#             }
#         },
#         'interface': 'self.debater(problem="", proposed_solutions=[])',
#         'output': {
#             'feedback': {
#                 'type': 'string',
#                 'desc': 'The feedback or insights provided by the debater.'
#             },
#             'solution': {
#                 'type': 'string',
#                 'desc': 'The synthesized final conclusion after debating and reasoning comparison.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.debater = operator.Debater(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         num_round = 3
#         num_debater = 3
#         proposed_solutions = []
#         for r in range(num_round):
#             proposed_solution_round = []
#             for i in range(num_debater):
#                 solution = await self.debater(problem=problem, proposed_solutions=proposed_solutions[r-1])
        
#                 step_x_solution = solution['solution']
#                 step_x_feedback = solution['feedback']
                
#                 proposed_solution_round.append(step_x_solution)
#             proposed_solutions.append(proposed_solution_round)
#         """
#     },
#     'Judge': {
#         'description': "Evaluates multiple solutions or arguments (e.g., from debaters or ensemble outputs) and decides the most valid or correct one according to predefined judging criteria.",
#         'input': {
#             'problem': {
#                 'type': 'string',
#                 'desc': 'The topic or problem under evaluation.'
#             },
#             'solutions': {
#                 'type': 'list[str]',
#                 'desc': 'A set of competing answers or reasoning paths to evaluate.'
#             }
#         },
#         'interface': 'self.judge(problem="", solutions=[])',
#         'output': {
#             'justification': {
#                 'type': 'string',
#                 'desc': 'The justification or reasoning behind the selected solution.'
#             }, 
#             'best_solution': {
#                 'type': 'string',
#                 'desc': 'The final decision after judging the proposed solutions.'
#             }
#         },
#         'example': """
# class Workflow:
#     def __init__(
#         self,
#         ....
#     ) -> None:
#         ....
#         self.debater = operator.Debater(self.llm)
#         self.judge = operator.Judge(self.llm)

#     async def __call__(self, problem: str):    
#         # Step x: .....
#         num_round = 3
#         num_debater = 3
#         proposed_solutions = []
#         for r in range(num_round):
#             proposed_solution_round = []
#             for i in range(num_debater):
#                 solution = await self.debater(problem=problem, proposed_solutions=proposed_solutions[r-1])
        
#                 step_x_solution = solution['solution']
#                 step_x_feedback = solution['feedback']
                
#                 proposed_solution_round.append(step_x_solution)
#             proposed_solutions.append(proposed_solution_round)
            
#         solution = await self.judge(problem=problem, solutions=proposed_solutions[num_round-1])
        
#         step_x_solution = solution['best_solution']
#         step_x_justification = solution['justification']
#         """
#     }
# }

OPERATORS_LIST = {
    'Custom': {
        'description': "A single agent with a customizable prompt, which is used in general objective.",
        'input': {
            'input': {
                'type': 'string',
                'desc': 'The string describing the problem that needs to be solved.',
            },
            'instruction': {
                'type': 'string',
                'desc': 'The instruction of addressing this problem.'
            }
        },
        'interface': 'self.custom(input="", instruction="")',
        'output': {
            'response': {
                'type': 'string',
                'desc': 'The response generated by the agent.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        # Step x: .....
        solution, logs = await self.custom(input=problem, instruction="")
        invoking_logs.append(logs)
        
        step_x_response = solution['response']
        """
    },
    'AnswerGenerate': {
        'description': "Generate a direct answer based on the given question or input text using the predefined answer generation prompt.",
        'input': {
            'input': {
                'type': 'string',
                'desc': 'The question or problem statement to be answered.'
            }
        },
        'interface': 'self.answer_generate(input="")',
        'output': {
            'thought': {
                'type': 'string',
                'desc': 'The thought process or reasoning behind the generated answer.'
            },
            'answer': {
                'type': 'string',
                'desc': 'The generated direct answer to the question or input text.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.answer_generate = operator.AnswerGenerate(self.llm)

    async def __call__(self, problem: str):
        # Step x: .....
        solution, logs = await self.answer_generate(input=problem)
        invoking_logs.append(logs)
        
        step_x_thought = solution['thought']
        step_x_answer = solution['answer']
        """
    },
    'CustomCodeGenerate': {
        'description': "Generate executable source code from a problem description and an entry point specification, following custom instructions.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem description or context for the code generation task.'
            },
            'entry_point': {
                'type': 'string',
                'desc': 'The name of the entry point function to be implemented in the generated code.'
            },
            'instruction': {
                'type': 'string',
                'desc': 'Instructions or prompts for the code generation task.'
            }
        },
        'interface': 'self.custom_code_generate(problem="", entry_point="", instruction="")',
        'output': {
            'code': {
                'type': "string",
                'desc': "The generated Python executable source code."
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)

    async def __call__(self, problem: str):
        # Step x: .....
        solution, logs = await self.custom_code_generate(problem=problem, entry_point="solve", instruction="")
        invoking_logs.append(logs)
        
        step_x_code = solution['code']
        """
    },
    'ScEnsemble': {
        'description': "Implements Self-Consistency (SC) Ensemble reasoning â€” aggregates multiple reasoning paths and selects the most consistent or common solution.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem statement or question for which SC ensemble reasoning is applied.'
            },
            'solutions': {
                'type': 'list[str]',
                'desc': 'A list of candidate solutions or reasoning paths for the problem.'
            }
        },
        'interface': 'self.sc_ensemble(solutions=[], problem="")',
        'output': {
            'thought': {
                'type': 'string',
                'desc': 'The thought process or reasoning behind the selected solution.'
            },
            'sc_solution': {
                'type': 'string',
                'desc': 'The selected most consistent or common solution.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.sc_ensemble = operator.ScEnsemble(self.llm)
        self.answer_generate = operator.AnswerGenerate(self.llm)

    async def __call__(self, problem: str):
    
        solutions = []
        
        for i in range(3):
            # Step x - 1: .....
            solution, logs = await self.answer_generate(input=problem)
            invoking_logs.append(logs)
            step_x-1_thought = solution['thought']
            step_x-1_answer = solution['answer']
            
            solutions = solutions + [step_x-1_answer]
    
        # Step x: .....
        solution, logs = await self.sc_ensemble(problem=problem, solutions=solutions)
        invoking_logs.append(logs)
        
        step_x_thought = solution['thought']
        step_x_sc_solution = solution['sc_solution']
        """
    },
    'Programmer': {
        'description': "A reasoning agent that generates, executes, and iteratively improves Python code solutions through feedback loops and retry mechanisms.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem or coding task to be solved.'
            },
            'analysis': {
                'type': 'string',
                'desc': 'Optional contextual analysis or explanation of the problem to guide generation.'
            }
        },
        'interface': 'self.programmer(problem="", analysis="None")',
        'output': {
            'code': {
                'type': 'string',
                'desc': 'The generated Python code solution.'
            },
            'output': {
                'type': 'string',
                'desc': 'The output or result of executing the generated code.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.programmer = operator.Programmer(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        solution, logs = await self.programmer(problem=problem, analysis="")
        invoking_logs.append(logs)
        
        step_x_code = solution['code']
        step_x_output = solution['output']
        """
    },
    'Test': {
        'description': "Executes public test cases for a given solution. If tests fail, the model reflects on the errors and revises the code until it passes or the retry limit is reached.",
        'input': {
            'problem': 'The original problem or coding task.',
            'solution': 'The candidate Python code solution to be tested.',
            'entry_point': 'The name of the function to be executed in the test environment.'
        },
        'interface': 'self.test(problem="", solution="", entry_point="")',
        'output': "An object containing the test 'result' (True/False) and possibly a revised 'solution'."
    },
    'Format': {
        'description': "Formats a generated solution into a clean and well-presented format (e.g., readable structure, docstring, or comment formatting).",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem or coding task.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The unformatted or raw generated solution.'
            }
        },
        'interface': 'self.format(problem="", solution="")',
        'output': {
            'solution': {
                'type': 'string',
                'desc': 'The formatted and polished solution text.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.format = operator.Format(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        solution, logs = await self.format(problem=problem, solution=".....")
        invoking_logs.append(logs)
        
        step_x_solution = solution['solution']
        """
    },
    'Review': {
        'description': "Performs critical review on a given solution, evaluating its correctness, completeness, and efficiency, and producing structured feedback.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem statement.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The proposed solution or reasoning to review.'  
            }
        },
        'interface': 'self.review(problem="", solution="")',
        'output': {
            'review_result': {
                'type': 'boolean',
                'desc': 'The Review Result (Bool). If you think this solution looks good for you, return "true"; If not, return "false".'
            },
            'feedback': {
                'type': 'string',
                'desc': 'Your FeedBack for this problem based on the criteria. If the review result is true, you can put it "nothing here".'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.review = operator.Review(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        solution, logs = await self.review(problem=problem, solution=".....")
        invoking_logs.append(logs)
        
        step_x_review_result = solution['review_result']
        step_x_feedback = solution['feedback']
        """
    },
    'Revise': {
        'description': "Refines a solution based on feedback from the review process to produce an improved version addressing identified issues.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem statement.'  
            },
            'solution': {
                'type': 'string',
                'desc': 'The proposed solution or reasoning to refine.'
            },
            'feedback':{
                'type': 'string',
                'desc': 'Reviewer or critic feedback indicating issues or improvements.'
            }
        },
        'interface': 'self.revise(problem="", solution="", feedback="")',
        'output': {
            'solution': {
                'type': 'string',
                'desc': 'Based on the feedback, revised solution for this problem'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.revise = operator.Revise(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        solution, logs = await self.revise(problem=problem, solution=".....", feedback="....")
        invoking_logs.append(logs)
        
        step_x_solution = solution['solution']
        """
    },
    'MdEnsemble': {
        'description': "Implements majority voting (multi-decision ensemble) reasoning by sampling multiple model outputs and selecting the most frequently chosen answer.",
        'input': {
            'solutions': 'A list of candidate answers or solutions generated by different agents or runs.',
            'problem': 'The problem statement or question for which ensemble voting is applied.',
            'mode': 'Optional XML/text output mode.'
        },
        'interface': 'self.md_ensemble(solutions=[], problem="")',
        'output': 'The final selected solution after majority voting aggregation.'
    },
    'Debater': {
        'description': "An agent who argues for and against proposed solutions from other debaters to refine reasoning and reveal the most robust argument.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The topic or problem under debate.'
            },
            'proposed_solutions': {
                'type': 'list[str]',
                'desc': 'The proposed solutions or arguments from other debaters.'
            }
        },
        'interface': 'self.debater(problem="", proposed_solutions=[])',
        'output': {
            'feedback': {
                'type': 'string',
                'desc': 'The feedback or insights provided by the debater.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The synthesized final conclusion after debating and reasoning comparison.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.debater = operator.Debater(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        num_round = 3
        num_debater = 3
        proposed_solutions = []
        for r in range(num_round):
            proposed_solution_round = []
            for i in range(num_debater):
                solution, logs = await self.debater(problem=problem, proposed_solutions=proposed_solutions[r-1])
                invoking_logs.append(logs)

                step_x_solution = solution['solution']
                step_x_feedback = solution['feedback']
                
                proposed_solution_round.append(step_x_solution)
            proposed_solutions.append(proposed_solution_round)
        """
    },
    'Judge': {
        'description': "Evaluates multiple solutions or arguments (e.g., from debaters or ensemble outputs) and decides the most valid or correct one according to predefined judging criteria.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The topic or problem under evaluation.'
            },
            'solutions': {
                'type': 'list[str]',
                'desc': 'A set of competing answers or reasoning paths to evaluate.'
            }
        },
        'interface': 'self.judge(problem="", solutions=[])',
        'output': {
            'justification': {
                'type': 'string',
                'desc': 'The justification or reasoning behind the selected solution.'
            }, 
            'best_solution': {
                'type': 'string',
                'desc': 'The final decision after judging the proposed solutions.'
            }
        },
        'example': """
class Workflow:
    def __init__(
        self,
        ....
    ) -> None:
        ....
        self.debater = operator.Debater(self.llm)
        self.judge = operator.Judge(self.llm)

    async def __call__(self, problem: str):    
        # Step x: .....
        num_round = 3
        num_debater = 3
        proposed_solutions = []
        for r in range(num_round):
            proposed_solution_round = []
            for i in range(num_debater):
                solution, logs = await self.debater(problem=problem, proposed_solutions=proposed_solutions[r-1])
                invoking_logs.append(logs)
        
                step_x_solution = solution['solution']
                step_x_feedback = solution['feedback']
                
                proposed_solution_round.append(step_x_solution)
            proposed_solutions.append(proposed_solution_round)
            
        solution, logs = await self.judge(problem=problem, solutions=proposed_solutions[num_round-1])
        invoking_logs.append(logs)
        
        step_x_solution = solution['best_solution']
        step_x_justification = solution['justification']
        """
    }
}


MAS_TEMPLATE = '''
class Workflow:
    def __init__(
        self,
        name: str,
        llm_config,
        dataset: DatasetType,
    ) -> None:
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        """
        Implementation of the graph
        """
        
        invoking_logs = []
        
        solution, log = await self.custom(input=problem, instruction="")
        invoking_logs.append(log)
        
        return solution['response'], invoking_logs
'''
