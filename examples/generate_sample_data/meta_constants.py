OPERATORS_LIST = {
    'Custom': {
        'description': "A single agent with a customizable prompt, which is used in general objective.",
        'input': {
            'input': {
                'type': 'string',
                'desc': 'The string describing the problem that needs to be solved.',
            },
            'instruction': {
                'type': 'string',
                'desc': 'The instruction of addressing this problem.'
            }
        },
        'interface': 'self.custom(input="", instruction="")',
        'output': {
            'response': {
                'type': 'string',
                'desc': 'The response generated by the agent.'
            }
        }
    },
    'AnswerGenerate': {
        'description': "Generate a direct answer based on the given question or input text using the predefined answer generation prompt.",
        'input': {
            'input': {
                'type': 'string',
                'desc': 'The question or problem statement to be answered.'
            }
        },
        'interface': 'self.answer_generate(input="")',
        'output': {
            'thought': {
                'type': 'string',
                'desc': 'The thought process or reasoning behind the generated answer.'
            },
            'answer': {
                'type': 'string',
                'desc': 'The generated direct answer to the question or input text.'
            }
        }
    },
    'CustomCodeGenerate': {
        'description': "Generate executable source code from a problem description and an entry point specification, following custom instructions.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem description or context for the code generation task.'
            },
            'entry_point': {
                'type': 'string',
                'desc': 'The name of the entry point function to be implemented in the generated code.'
            },
            'instruction': {
                'type': 'string',
                'desc': 'Instructions or prompts for the code generation task.'
            }
        },
        'interface': 'self.custom_code_generate(problem="", entry_point="", instruction="")',
        'output': {
            'code': {
                'type': "string",
                'desc': "The generated Python executable source code."
            }
        }
    },
    'ScEnsemble': {
        'description': "Implements Self-Consistency (SC) Ensemble reasoning â€” aggregates multiple reasoning paths and selects the most consistent or common solution.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem statement or question for which SC ensemble reasoning is applied.'
            },
            'solutions': {
                'type': 'list[str]',
                'desc': 'A list of candidate solutions or reasoning paths for the problem.'
            }
        },
        'interface': 'self.sc_ensemble(solutions=[], problem="")',
        'output': {
            'thought': {
                'type': 'string',
                'desc': 'The thought process or reasoning behind the selected solution.'
            },
            'sc_solution': {
                'type': 'string',
                'desc': 'The selected most consistent or common solution.'
            }
        }
    },
    'Programmer': {
        'description': "A reasoning agent that generates, executes, and iteratively improves Python code solutions through feedback loops and retry mechanisms.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The problem or coding task to be solved.'
            },
            'analysis': {
                'type': 'string',
                'desc': 'Optional contextual analysis or explanation of the problem to guide generation.'
            }
        },
        'interface': 'self.programmer(problem="", analysis="None")',
        'output': {
            'code': {
                'type': 'string',
                'desc': 'The generated Python code solution.'
            },
            'output': {
                'type': 'string',
                'desc': 'The output or result of executing the generated code.'
            }
        }
    },
    'Test': {
        'description': "Executes public test cases for a given solution. If tests fail, the model reflects on the errors and revises the code until it passes or the retry limit is reached.",
        'input': {
            'problem': 'The original problem or coding task.',
            'solution': 'The candidate Python code solution to be tested.',
            'entry_point': 'The name of the function to be executed in the test environment.'
        },
        'interface': 'self.test(problem="", solution="", entry_point="")',
        'output': "An object containing the test 'result' (True/False) and possibly a revised 'solution'."
    },
    'Format': {
        'description': "Formats a generated solution into a clean and well-presented format (e.g., readable structure, docstring, or comment formatting).",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem or coding task.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The unformatted or raw generated solution.'
            }
        },
        'interface': 'self.format(problem="", solution="")',
        'output': {
            'solution': {
                'type': 'string',
                'desc': 'The formatted and polished solution text.'
            }
        }
    },
    'Review': {
        'description': "Performs critical review on a given solution, evaluating its correctness, completeness, and efficiency, and producing structured feedback.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem statement.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The proposed solution or reasoning to review.'  
            }
        },
        'interface': 'self.review(problem="", solution="")',
        'output': {
            'review_result': {
                'type': 'boolean',
                'desc': 'The Review Result (Bool). If you think this solution looks good for you, return "true"; If not, return "false".'
            },
            'feedback': {
                'type': 'string',
                'desc': 'Your FeedBack for this problem based on the criteria. If the review result is true, you can put it "nothing here".'
            }
        }
    },
    'Revise': {
        'description': "Refines a solution based on feedback from the review process to produce an improved version addressing identified issues.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The original problem statement.'  
            },
            'solution': {
                'type': 'string',
                'desc': 'The proposed solution or reasoning to refine.'
            },
            'feedback':{
                'type': 'string',
                'desc': 'Reviewer or critic feedback indicating issues or improvements.'
            }
        },
        'interface': 'self.revise(problem="", solution="", feedback="")',
        'output': {
            'solution': {
                'type': 'string',
                'desc': 'Based on the feedback, revised solution for this problem'
            }
        }
    },
    'MdEnsemble': {
        'description': "Implements majority voting (multi-decision ensemble) reasoning by sampling multiple model outputs and selecting the most frequently chosen answer.",
        'input': {
            'solutions': 'A list of candidate answers or solutions generated by different agents or runs.',
            'problem': 'The problem statement or question for which ensemble voting is applied.',
            'mode': 'Optional XML/text output mode.'
        },
        'interface': 'self.md_ensemble(solutions=[], problem="")',
        'output': 'The final selected solution after majority voting aggregation.'
    },
    'Debater': {
        'description': "An agent who argues for and against proposed solutions from other debaters to refine reasoning and reveal the most robust argument.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The topic or problem under debate.'
            },
            'proposed_solutions': {
                'type': 'list[str]',
                'desc': 'The proposed solutions or arguments from other debaters.'
            }
        },
        'interface': 'self.debater(problem="", proposed_solutions=[])',
        'output': {
            'feedback': {
                'type': 'string',
                'desc': 'The feedback or insights provided by the debater.'
            },
            'solution': {
                'type': 'string',
                'desc': 'The synthesized final conclusion after debating and reasoning comparison.'
            }
        }
    },
    'Judge': {
        'description': "Evaluates multiple solutions or arguments (e.g., from debaters or ensemble outputs) and decides the most valid or correct one according to predefined judging criteria.",
        'input': {
            'problem': {
                'type': 'string',
                'desc': 'The topic or problem under evaluation.'
            },
            'solutions': {
                'type': 'list[str]',
                'desc': 'A set of competing answers or reasoning paths to evaluate.'
            }
        },
        'interface': 'self.judge(problem="", solutions=[])',
        'output': {
            'justification': {
                'type': 'string',
                'desc': 'The justification or reasoning behind the selected solution.'
            }, 
            'best_solution': {
                'type': 'string',
                'desc': 'The final decision after judging the proposed solutions.'
            }
        }
    }
}


MAS_TEMPLATE = '''
from typing import Literal
import metagpt.ext.aflow.scripts.optimized.DROP.graphs.template.operator as operator
import metagpt.ext.aflow.scripts.optimized.DROP.graphs.round_1.prompt as prompt_custom
from metagpt.provider.llm_provider_registry import create_llm_instance
from metagpt.utils.cost_manager import CostManager
from metagpt.configs.models_config import ModelsConfig
import asyncio

DatasetType = Literal["HumanEval", "MBPP", "GSM8K", "MATH", "HotpotQA", "DROP"]

class Workflow:
    def __init__(
        self,
        name: str,
        llm_config,
        dataset: DatasetType,
    ) -> None:
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        """
        Implementation of the graph
        """
        solution = await self.custom(input=problem, instruction="")
        return solution['response']
'''
